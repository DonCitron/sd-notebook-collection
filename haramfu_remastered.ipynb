{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/sd-notebook-collection/blob/main/haramfu_remastered.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "  <summary><b>Credits</b></summary>\n",
        "\tThis colab notebook was hardly inspired by:\n",
        "\n",
        "- <small>[Nocrypt's Colab Remastered](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq)</small>\n",
        "- <small>[Acheong's Diffusion Web UI](https://colab.research.google.com/github/acheong08/Diffusion-ColabUI/blob/main/Diffusion_WebUI.ipynb)</small>\n",
        "\n",
        "</details>\n",
        "\n",
        "![](https://visitor-badge.glitch.me/badge?page_id=linaqruf.haramfu) [![](https://dcbadge.vercel.app/api/shield/931591564424253512?style=flat)](https://lookup.guru/931591564424253512) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/linaqruf) <a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white\"/></a>\n",
        "# **Haramfu Remastered**\n",
        "A Customizable N__FU <small><font color=gray>(He-Who-Must-Not-Be-Named)</small></font>\n",
        "\n",
        "<small><font color=gray>v2.080223 | [For ppl who copied this colab, check updates here](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq)</small></font>\n",
        "\n"
      ],
      "metadata": {
        "id": "qw4DdjTkPqJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## üöÄ Start `Haramfu`\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from subprocess import getoutput\n",
        "from google.colab import drive\n",
        "from datetime import timedelta\n",
        "from urllib.parse import unquote\n",
        "\n",
        "%cd /content\n",
        "try:\n",
        "  start_colab\n",
        "except:\n",
        "  start_colab = int(time.time())-5\n",
        "clear_output()\n",
        "print(\"\\033[93m\") #Yellow text\n",
        "\n",
        "# Check if gpu exist, stop if don't.\n",
        "try:\n",
        "  output \n",
        "except:\n",
        "  print('‚åö Checking GPU...', end='')\n",
        "  output = getoutput('nvidia-smi --query-gpu=gpu_name --format=csv')\n",
        "  if \"name\" in output:\n",
        "    gpu_name = output[5:]\n",
        "    print('\\r‚úÖ Current GPU:', gpu_name, flush=True)\n",
        "  else:\n",
        "    print('\\r\\033[91m‚ùé ERROR: No GPU detected. Please do step below to enable.\\n', flush=True)\n",
        "    display(HTML(\"<img src='https://i.ibb.co/HC9KH17/NVIDIA-Share-23-01-02-173037.png' width='800px'/>\"))\n",
        "    print('\\033[91m\\nIf it says \"Cannot connect to GPU backend\", meaning you\\'ve either reached free usage limit. OR there\\'s no gpu available.\\n\\nDon\\'t mind me... I\\'m destroying your current session for your own good...')\n",
        "    display(HTML(\"<img src='https://media.tenor.com/E9omRGF7x0AAAAAC/hitori-gotou-bocchi-rock.gif' width='500px'/>\"))\n",
        "    time.sleep(5)\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "\n",
        "#@markdown ## **Configuration**\n",
        "load_in_vram = True #@param {type:'boolean'}\n",
        "install_xformers = True #@param {type:'boolean'}\n",
        "use_ema = True #@param {type:\"boolean\"}\n",
        "use_precision = \"float16\" #@param [\"float16\", \"float32\"]\n",
        "save_output = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ## <br> **Select Available Tunnel**\n",
        "#@markdown <small><font color=gray> **HINT**: ngrok > trycloudflare > bore\n",
        "#@markdown > Get <b>your</b> token for ngrok [here](https://dashboard.ngrok.com/get-started/your-authtoken) \n",
        "ngrok_token = \"\" #@param {type:\"string\"}\n",
        "cloudflared_tunnel = True #@param {type:\"boolean\"}\n",
        "with_bore = False #@param {type:\"boolean\"}\n",
        "\n",
        "start_install = int(time.time())\n",
        "if not os.path.isdir(\"/content/haramfu\"):\n",
        "  print(\"üöÄ Installing dependencies... Please do not stop this process at all cost...\", end='')\n",
        "  with capture.capture_output() as cap:\n",
        "    !apt-get install -y lz4\n",
        "    !apt -q install liblz4-tool aria2\n",
        "\n",
        "    user_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d \"/content/\" -o \"haramfu.tar.lz4\" \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/haramfu/haramfu.tar.lz4\"\n",
        "    !tar -xI lz4 -xvf \"/content/haramfu.tar.lz4\" --directory=/ #/content/haramfuu\n",
        "    os.remove(\"/content/haramfu.tar.lz4\")\n",
        "\n",
        "    %cd /content/haramfu\n",
        "    !pip install -q -r requirements.txt\n",
        "    !pip install -q safetensors pyngrok pytorch_lightning==1.7.7 Pillow==9.1.0\n",
        "    del cap \n",
        "  print(\"\\rüöÄ Finished unpacking.\\n\", end='', flush=True)\n",
        "  \n",
        "else:\n",
        "  print(\"üöÄ Already unpacked... Skipping.\")\n",
        "  time_since_start = timedelta(seconds=time.time()-start_colab)\n",
        "  print(\"‚åö You've been running this colab for\",\"%02d:%02d:%02d\" % (time_since_start.seconds / 3600, (time_since_start.seconds / 60) % 60, time_since_start.seconds % 60))\n",
        "\n",
        "  with capture.capture_output() as cap:\n",
        "    %cd /content/\n",
        "    s = getoutput('nvidia-smi')\n",
        "    if install_xformers:\n",
        "      if 'T4' in s:\n",
        "        %pip install https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "      if 'A100' in s:\n",
        "        %pip install https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15+e163309.d20230103.ColabProA100-cp38-cp38-linux_x86_64.whl\n",
        "    \n",
        "    del cap\n",
        "#@markdown ## <br> **Download Models**\n",
        "#@markdown <small><font color=gray> **HINT**: if **customUrl** is empty, **modelName** is activated instead</small>\n",
        "installModels = []\n",
        "installVae = []\n",
        "\n",
        "#@markdown ### Available Model\n",
        "\n",
        "\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/Anything-V3.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-2.safetensors\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/pastel-mix/resolve/main/pastelmix-fp32.ckpt\", \\\n",
        "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
        "            \"https://huggingface.co/Linaqruf/hitokomoru-diffusion/resolve/main/hitokomoru-30000.ckpt\", \\\n",
        "            \"https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/Counterfeit-V2.5_pruned.safetensors\", \\\n",
        "            \"https://huggingface.co/SweetLuna/Kenshi/resolve/main/KENSHI%2001/KENSHI01_Pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "            \"animefull-final-pruned\", \\\n",
        "            \"anything-v3-0\", \\\n",
        "            \"anything-v3-2\", \\\n",
        "            \"anything-v4-0\", \\\n",
        "            \"anything-v4.5\", \\\n",
        "            \"pastelmix\", \\\n",
        "            \"AbyssOrangeMix2_nsfw\", \\\n",
        "            \"hitokomoru\", \\\n",
        "            \"Counterfeit-V2.5\", \\\n",
        "            \"KENSHI\", \\\n",
        "            \"Kani-anime\", \\\n",
        "            \"dpepteahand3\"]\n",
        "modelName = \"pastelmix\"  #@param [\"\", \"animefull-final-pruned\", \"anything-v3-0\", \"anything-v3-2\", \"anything-v4-0\", \"anything-v4.5\", \"pastelmix\", \"AbyssOrangeMix2_nsfw\", \"hitokomoru\", \"Counterfeit-V2.5\", \"KENSHI\", \"Kani-anime\", \"dpepteahand3\"]\n",
        "\n",
        "#@markdown ### Custom model\n",
        "#@markdown <small><font color=gray> **HINT**: check for models in [Civitai](https://civitai.com/) or [Huggingface]() </small><br>\n",
        "#@markdown <small><font color=gray> **HINT**: **LoRA, TI, and Hypernetwork** is not supported, so don't even try to download them </small><br>\n",
        "#@markdown <small><font color=gray> **HINT**: indirect support for `.safetensors`, downloaded safetensors model will automatically converted to `.ckpt` </small><br>\n",
        "customUrl = \"\" #@param {'type': 'string'}\n",
        "if customUrl.endswith(\"ckpt\") or customUrl.endswith(\"safetensors\"):\n",
        "  base_name = os.path.splitext(os.path.basename(customUrl))[0]\n",
        "else:\n",
        "  base_name = os.path.basename(customUrl)\n",
        "\n",
        "#@markdown #### Available VAE\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "if modelName != \"\":\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "def is_safetensors(path):\n",
        "  return os.path.splitext(path)[1].lower() == '.safetensors'\n",
        "\n",
        "def download(name, url, is_vae:bool):\n",
        "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
        "\n",
        "  model_path = f\"/content/haramfu/models/{name}/model.{ext}\"\n",
        "  model_path_ckpt = f\"/content/haramfu/models/{name}/model.ckpt\"\n",
        "\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "\n",
        "  if not is_vae:\n",
        "    if not os.path.exists(f'/content/config.yaml'):\n",
        "      !wget https://huggingface.co/Daswer123/gfdsa/raw/main/sfw.yaml -O /content/config.yaml\n",
        "    \n",
        "    os.makedirs(f'/content/haramfu/models/{name}', exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(f'/content/haramfu/models/{name}/config.yaml'):\n",
        "      !cp /content/config.yaml /content/haramfu/models/{name}\n",
        "    \n",
        "    if url.startswith(\"https://drive.google.com\"):\n",
        "      %cd /content/haramfu/models/{name}\n",
        "      !gdown --fuzzy {url} -O model.ckpt\n",
        "    elif url.startswith(\"https://huggingface.co/\"):\n",
        "      if '/blob/' in url:\n",
        "        url = url.replace('/blob/', '/resolve/')\n",
        "      !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/haramfu/models/{name} -o model.{ext} \"{url}\"\n",
        "    else:\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d /content/haramfu/models/{name} -o model.{ext} \"{url}\"\n",
        "    \n",
        "    if is_safetensors(model_path):\n",
        "      from torch import save\n",
        "      from safetensors.torch import load_file\n",
        "      weights = load_file(model_path, device=\"cpu\")\n",
        "      save(weights, model_path_ckpt)\n",
        "      os.remove(model_path)\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/haramfu/models -o {name} \"{url}\"\n",
        "\n",
        "def run_download():\n",
        "  print(f\"üèÅ Downloading Models and VAE.... Please wait...\", end='')\n",
        "  with capture.capture_output() as cap:\n",
        "    if customUrl !=\"\":\n",
        "      download(base_name, customUrl, False)\n",
        "    else:\n",
        "      for model in installModels:\n",
        "        download(model[0], model[1], False)\n",
        "    if vaeName != \"none\":\n",
        "      for vae in installVae:\n",
        "        download(vae[0], vae[1], True)\n",
        "    del cap\n",
        "run_download()\n",
        "\n",
        "if customUrl !=\"\":\n",
        "  modelName = base_name \n",
        "\n",
        "install_time = timedelta(seconds=time.time()-start_install)\n",
        "print(f\"\\rüèÅ Finished downloading {modelName} and {vaeName}.\", end='', flush=True)\n",
        "print(f\"\\rüöÄ Installation Completed. Took\",\"%02d:%02d:%02d ‚ö°\\n\" % (install_time.seconds / 3600, (install_time.seconds / 60) % 60, install_time.seconds % 60), end='', flush=True)\n",
        "\n",
        "if ngrok_token:\n",
        "  from pyngrok import ngrok\n",
        "  ngrok.set_auth_token(ngrok_token)\n",
        "  ngrok.kill()\n",
        "  public_url = ngrok.connect(6969).public_url\n",
        "  !nohup lt -l 0.0.0.0 -p 6969 > /content/nohup.out 2>&1 &  \n",
        "  print(\"‚úÖ Your ngrok link:\")\n",
        "  print(public_url)\n",
        "else:\n",
        "  with capture.capture_output() as cap:\n",
        "    if with_bore:\n",
        "      if not os.path.exists('/usr/bin/bore'):\n",
        "        !curl -Ls https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz | tar zx -C /usr/bin\n",
        "    if cloudflared_tunnel:\n",
        "      if not os.path.exists('/usr/bin/cloudflared'):\n",
        "        !curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared\n",
        "    del cap\n",
        "\n",
        "print(\"üí™ Wait until this line printed\")\n",
        "print(\"- INFO:     Uvicorn running on http://0.0.0.0:6969 (Press CTRL+C to quit)\")\n",
        "print(\"\\033[95m\")\n",
        "\n",
        "if load_in_vram:\n",
        "  !sed -i 's/map_location=\"cpu\"/map_location=\"cuda\"/g' /content/haramfu/hydra_node/models.py\n",
        "else:\n",
        "  !sed -i 's/map_location=\"cuda\"/map_location=\"cpu\"/g' /content/haramfu/hydra_node/models.py\n",
        "\n",
        "print(\"\\033[93mDon't see your files in here? Check your URL twice!\\n\")\n",
        "print(\"\\033[92m\\033[1m‚ï≠-üì¶ Models\\033[96m\")\n",
        "print(f\"- {modelName}\") \n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ VAEs\\033[96m\")\n",
        "print(f\"- {vaeName}\") \n",
        "print(\"\\n\")\n",
        "\n",
        "%cd /content/haramfu\n",
        "haramfu_cmd=f\"\"\"\n",
        "export DTYPE={use_precision}\n",
        "export CLIP_CONTEXTS=3\n",
        "export AMP=1\n",
        "export MODEL=stable-diffusion\n",
        "export DEV=True\n",
        "export MODEL_PATH=models/{modelName}\n",
        "export ENABLE_EMA={format(1) if use_ema else format(0)}\n",
        "export VAE_PATH=models/{vaeName}\n",
        "export PENULTIMATE=1\n",
        "export PYTHONDONTWRITEBYTECODE=1\n",
        "export SAVE_FILES={format(1) if save_output else format(0)}\n",
        "\n",
        "python -m uvicorn --host 0.0.0.0 --port=6969 main:app {\"& bore local 6969 --to bore.pub\" if with_bore and ngrok_token ==\"\" else \"\"} {\"& cloudflared tunnel --url localhost:6969\" if cloudflared_tunnel and ngrok_token == \"\" else \"\"}\n",
        "\"\"\"\n",
        "\n",
        "f = open(\"./run_haramfu.sh\", \"w\")\n",
        "f.write(haramfu_cmd)\n",
        "f.close()\n",
        "!chmod +x ./run_haramfu.sh\n",
        "!./run_haramfu.sh\n",
        "\n",
        "time_since_start = timedelta(seconds=time.time()-start_colab)\n",
        "print(\"\\n\\n\\033[93m‚åö You've been running this colab for\",\"%02d:%02d:%02d\" % (time_since_start.seconds / 3600, (time_since_start.seconds / 60) % 60, time_since_start.seconds % 60))\n",
        "print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "vv4xgT5-Slqi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  ## üìù Download Generated Images\n",
        "#@markdown Download file manually from files tab or save to Google Drive\n",
        "%cd /content/haramfu/\n",
        "\n",
        "!zip -r /content/images.zip images\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "def create_folder(folder_name):\n",
        "    # Check if folder exists\n",
        "    file_list = drive.ListFile({'q': \"title='{}' and mimeType='application/vnd.google-apps.folder' and trashed=false\".format(folder_name)}).GetList()\n",
        "    if len(file_list) > 0:\n",
        "        # Folder exists\n",
        "        print('Debug: Folder exists')\n",
        "        folder_id = file_list[0]['id']\n",
        "    else:\n",
        "        print('Debug: Creating folder')\n",
        "        file = drive.CreateFile({'title': folder_name, 'mimeType': 'application/vnd.google-apps.folder'})\n",
        "        file.Upload()\n",
        "        folder_id = file.attr['metadata']['id']\n",
        "    # return folder id\n",
        "    return folder_id\n",
        "# Upload file to Google Drive\n",
        "def upload_file(file_name, folder_id, save_as):\n",
        "    # Check if file exists\n",
        "    file_list = drive.ListFile({'q': \"title='{}' and trashed=false\".format(save_as)}).GetList()\n",
        "    if len(file_list) > 0:\n",
        "        print('Debug: File already exists')\n",
        "        # Change file name to avoid overwriting\n",
        "        save_as = save_as + ' (1)'\n",
        "    file = drive.CreateFile({'title': save_as, 'parents': [{'id': folder_id}]})\n",
        "    file.SetContentFile(file_name)\n",
        "    # Upload and set permission to public\n",
        "    file.Upload()\n",
        "    file.InsertPermission({'type': 'anyone', 'value': 'anyone', 'role': 'reader'})\n",
        "    # return file id\n",
        "    return file.attr['metadata']['id']\n",
        "\n",
        "use_drive = True #@param {type:\"boolean\"}\n",
        "folder_name = \"AI_pic_archive\" #@param {type: \"string\"}\n",
        "save_as = \"oni.zip\" #@param {type: \"string\"}\n",
        "\n",
        "if use_drive:\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  file_id = upload_file('/content/images.zip', create_folder(folder_name), save_as)\n",
        "  print(\"Your sharing link: https://drive.google.com/file/d/\" + file_id + \"/view?usp=sharing\")  "
      ],
      "metadata": {
        "id": "pR_dctS8dL6F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
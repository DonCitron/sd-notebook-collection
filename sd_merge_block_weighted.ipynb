{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WDPfF4uc5pd1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/sd-notebook-collection/blob/main/sd_merge_block_weighted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Installation"
      ],
      "metadata": {
        "id": "akFjukqz5PbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.1 Install Dependencies\n",
        "import os\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "colab_ram_patch = True\n",
        "\n",
        "# Check if the directory already exists\n",
        "if os.path.isdir('/content/kohya-trainer'):\n",
        "  %cd /content/kohya-trainer\n",
        "  print(\"This folder already exists, will do a !git pull instead\\n\")\n",
        "  !git pull\n",
        "else:\n",
        "  !git clone https://github.com/Linaqruf/kohya-trainer\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "if colab_ram_patch:\n",
        "  !sed -i \"s@cpu@cuda@\" \\\n",
        "  {repo_dir}/library/model_util.py\n",
        "\n",
        "  !sed -i \"s@cuda_count@cpu_count@\" \\\n",
        "  {repo_dir}/library/model_util.py\n",
        "\n",
        "#@markdown This will install required Python packages\n",
        "!pip -qqqq install --upgrade --no-cache-dir gdown\n",
        "!apt -qqqq install liblz4-tool aria2\n",
        "!pip -qqqq install --upgrade -r requirements.txt\n",
        "!pip -qqqq install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/models'):\n",
        "  os.makedirs('/content/models')\n"
      ],
      "metadata": {
        "id": "0BicRIFqIjg0",
        "outputId": "bf67ec68-a001-425a-bcfd-cdca5d33008d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'kohya-trainer'...\n",
            "remote: Enumerating objects: 988, done.\u001b[K\n",
            "remote: Counting objects: 100% (402/402), done.\u001b[K\n",
            "remote: Compressing objects: 100% (224/224), done.\u001b[K\n",
            "remote: Total 988 (delta 286), reused 230 (delta 178), pack-reused 586\u001b[K\n",
            "Receiving objects: 100% (988/988), 2.73 MiB | 8.19 MiB/s, done.\n",
            "Resolving deltas: 100% (597/597), done.\n",
            "/content/kohya-trainer\n",
            "sed: can't read {repo_dir}/library/model_util.py: No such file or directory\n",
            "sed: can't read {repo_dir}/library/model_util.py: No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/kohya-trainer\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.15.0\n",
            "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.2.1)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.6.0.66)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers[torch]==0.10.2\n",
            "  Downloading diffusers-0.10.2-py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 KB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.35.0\n",
            "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (2.9.1)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.2.6\n",
            "  Downloading safetensors-0.2.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (2.25.1)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow<2.11 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (2.9.2)\n",
            "Collecting tensorflow<2.11\n",
            "  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->-r requirements.txt (line 2)) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->-r requirements.txt (line 2)) (3.9.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.4.12->-r requirements.txt (line 14)) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->-r requirements.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 4)) (4.7.0.68)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 4)) (0.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations->-r requirements.txt (line 4)) (0.18.3)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning->-r requirements.txt (line 8)) (2022.11.0)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning->-r requirements.txt (line 8)) (4.4.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (1.51.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->-r requirements.txt (line 10)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->-r requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->-r requirements.txt (line 13)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->-r requirements.txt (line 13)) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->-r requirements.txt (line 13)) (1.24.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (15.0.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (2.2.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (0.29.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.11->-r requirements.txt (line 17)) (3.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (3.8.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate==0.15.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (2022.10.10)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 10)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 10)) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 4)) (3.1.0)\n",
            "Building wheels for collected packages: fairscale, library\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292864 sha256=5292856a0038c6a926146016eab4c52c660edeeccc3541546785f476ad3d5155\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/c3/6e/3fca67aaef3657c2266e9ee439b54f534f05967cd8774cc65b\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for library: filename=library-0.0.0-py3-none-any.whl size=32537 sha256=3895cb70a09bd51c8088b68d69a42c412c08d3de3aacf0dd6976f53f42e04d1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/91/79/27ff9f5781eca489906f9047be4854ff313f3a5cb28b7c2074\n",
            "Successfully built fairscale library\n",
            "Installing collected packages: tokenizers, safetensors, library, keras, flatbuffers, bitsandbytes, tensorflow-estimator, requests, opencv-python, ftfy, einops, torchmetrics, lightning-utilities, huggingface-hub, fairscale, accelerate, transformers, timm, diffusers, tensorboard, pytorch_lightning, albumentations, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed accelerate-0.15.0 albumentations-1.3.0 bitsandbytes-0.35.0 diffusers-0.10.2 einops-0.6.0 fairscale-0.4.4 flatbuffers-23.1.21 ftfy-6.1.1 huggingface-hub-0.12.0 keras-2.10.0 library-0.0.0 lightning-utilities-0.6.0.post0 opencv-python-4.7.0.68 pytorch_lightning-1.9.0 requests-2.28.2 safetensors-0.2.6 tensorboard-2.10.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 timm-0.4.12 tokenizers-0.13.2 torchmetrics-0.11.0 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.28.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 lz4\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 liblz4-tool lz4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 1,562 kB of archives.\n",
            "After this operation, 6,198 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libc-ares2 amd64 1.15.0-1ubuntu0.1 [38.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libaria2-0 amd64 1.35.0-1build1 [1,082 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 aria2 amd64 1.35.0-1build1 [356 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 lz4 amd64 1.9.2-2ubuntu0.20.04.1 [82.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 liblz4-tool all 1.9.2-2ubuntu0.20.04.1 [2,524 B]\n",
            "Fetched 1,562 kB in 2s (873 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 129502 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking aria2 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.2-2ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.2-2ubuntu0.20.04.1_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "Setting up lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Setting up aria2 (1.35.0-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.2. Download Available Model \n",
        "%cd /content/\n",
        "import os\n",
        "\n",
        "installModels = []\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "modelUrl = [\"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-latest.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3-better-vae/resolve/main/any-v3-fp32-better-vae.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.0.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/hesw23168/SD-Elysium-Model/resolve/main/Elysium_Anime_V2.ckpt\", \\\n",
        "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
        "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
        "modelList = [\"Animefull-final-latest\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Anything-V3\", \\\n",
        "             \"Anything-V3-pruned\", \\\n",
        "             \"Anything-V3-better-vae\", \\\n",
        "             \"Anything-V4\", \\\n",
        "             \"Anything-V4-pruned\", \\\n",
        "             \"Anything-V4-5-pruned\", \\\n",
        "             \"Kani-anime\", \\\n",
        "             \"Kani-anime-pruned\", \\\n",
        "             \"Elysium-anime-V2\", \\\n",
        "             \"OpenJourney-V2\", \\\n",
        "             \"Dreamlike-diffusion-V1-0\", \\\n",
        "             \"Stable-Diffusion-v1-5\"]\n",
        "modelName = \"Anything-V4\" #@param [\"Animefull-final-latest\", \"Animefull-final-pruned\", \"Anything-V3\", \"Anything-V3-pruned\", \"Anything-V3-better-vae\", \"Anything-V4\", \"Anything-V4-pruned\", \"Anything-V4-5-pruned\",\"Kani-anime\", \"Kani-anime-pruned\", \"Elysium-anime-V2\", \"OpenJourney-V2\", \"Dreamlike-diffusion-V1-0\", \"Stable-Diffusion-v1-5\"]\n",
        "\n",
        "installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o models/{checkpoint_name}.ckpt \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UKP8CU-KNkjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.3. Download Custom Model\n",
        "\n",
        "%cd /content/models\n",
        "\n",
        "import os\n",
        "\n",
        "#@markdown ### Custom model\n",
        "modelList = []\n",
        "modelUrl = \"https://huggingface.co/andite/pastel-mix/blob/main/pastelmix.ckpt\" #@param {'type': 'string'}\n",
        "modelUrl2 = \"https://huggingface.co/Linaqruf/hitokomoru-diffusion/resolve/main/hitokomoru-30000.ckpt\" #@param {'type': 'string'}\n",
        "modelUrl3 = \"\" #@param {'type': 'string'}\n",
        "modelUrl4 = \"\" #@param {'type': 'string'}\n",
        "\n",
        "modelList.extend([modelUrl, \n",
        "                  modelUrl2,\n",
        "                  modelUrl3,\n",
        "                  modelUrl4])\n",
        "\n",
        "def install_aria():\n",
        "  if not os.path.exists('/usr/bin/aria2c'):\n",
        "    !apt install -y -qq aria2\n",
        "\n",
        "def install(url):\n",
        "\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  \"{url}\"\n",
        "  elif url.startswith(\"magnet:?\"):\n",
        "    install_aria()\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 \"{url}\"\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    base_name = os.path.basename(url)\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d /content/models -o \"{base_name}\" \"{url}\"\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d /content/models -Z \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for customModel in modelList:\n",
        "    install(customModel)\n",
        "\n",
        "install_checkpoint()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A2NC0tm8vU-K",
        "outputId": "0e09a4dc-8306-4108-9b48-beffcc195344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models\n",
            " *** Download Progress Summary as of Fri Jan 27 02:34:04 2023 *** \n",
            "=\n",
            "[#a6b5ef 2.4GiB/5.5GiB(43%) CN:16 DL:249MiB ETA:12s]\n",
            "FILE: /content/models/pastelmix.ckpt\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Fri Jan 27 02:34:15 2023 *** \n",
            "=\n",
            "[#a6b5ef 4.8GiB/5.5GiB(86%) CN:16 DL:221MiB ETA:3s]\n",
            "FILE: /content/models/pastelmix.ckpt\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "a6b5ef|\u001b[1;32mOK\u001b[0m  |   203MiB/s|/content/models/pastelmix.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            " *** Download Progress Summary as of Fri Jan 27 02:34:32 2023 *** \n",
            "=\n",
            "[#8af57d 1.2GiB/5.1GiB(24%) CN:16 DL:160MiB ETA:25s]\n",
            "FILE: /content/models/hitokomoru-30000.ckpt\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Fri Jan 27 02:34:43 2023 *** \n",
            "=\n",
            "[#8af57d 3.8GiB/5.1GiB(73%) CN:16 DL:240MiB ETA:5s]\n",
            "FILE: /content/models/hitokomoru-30000.ckpt\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Fri Jan 27 02:34:53 2023 *** \n",
            "=\n",
            "[#8af57d 5.1GiB/5.1GiB(99%) CN:2 DL:138MiB]\n",
            "FILE: /content/models/hitokomoru-30000.ckpt\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "8af57d|\u001b[1;32mOK\u001b[0m  |   180MiB/s|/content/models/hitokomoru-30000.ckpt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Exception caught\n",
            "Exception: [download_helper.cc:451] errorCode=1 Unrecognized URI or unsupported protocol: \n",
            "\n",
            "Exception caught\n",
            "Exception: [download_helper.cc:451] errorCode=1 Unrecognized URI or unsupported protocol: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.4. Download VAE\n",
        "%cd /content/\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "45v0uhxVr9jE",
        "outputId": "c841fc78-2752-4110-c806-1b8804938463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "3d2d4c|\u001b[1;32mOK\u001b[0m  |   169MiB/s|/content/vae/anime.vae.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Block Weighted"
      ],
      "metadata": {
        "id": "Nli2x22bEiPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Variable\n",
        "model_1 = \"/content/models/pastelmix.ckpt\" #@param {type: \"string\"}\n",
        "model_2 = \"/content/models/hitokomoru-30000.ckpt\" #@param {type: \"string\"}\n",
        "output = \"/content/models/new.ckpt\" #@param {type: \"string\"}\n",
        "base_alpha = 1.0 #@param {type: \"number\"}\n",
        "verbose = True #@param {type: \"boolean\"}"
      ],
      "metadata": {
        "id": "VvQ-_qKZvrK-",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Manually Adjust UNet Layer Weight\n",
        "IN_00 = 1 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_01 = 0.9166666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_02 = 0.8333333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_03 = 0.75 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_04 = 0.6666666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_05 = 0.5833333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_06 = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_07 = 0.4166666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_08 = 0.3333333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_09 = 0.25 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_10 = 0.1666666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "IN_11 = 0.0833333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "M_00 = 0 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_11 = 0.0833333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_10 = 0.1666666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_09 = 0.25 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_08 = 0.3333333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_07 = 0.4166666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_06 = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_05 = 0.5833333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_04 = 0.6666666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_03 = 0.75 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_02 = 0.8333333333 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_01 = 0.9166666667 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}\n",
        "OUT_00 = 1 #@param {type:\"slider\", min:0, max:1, step:0.0000000001}"
      ],
      "metadata": {
        "id": "kThIIUYFwHre",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title I'M GOING TO MEERRRGGGEEE ARRGGRGHH\n",
        "mode = \"manual_adjust\" #@param [\"manual_adjust\",\"copy_paste\",\"use_preset\"]\n",
        "swap_model = False #@param {type:\"boolean\"}\n",
        "input_weight = \"\" #@param {type:\"string\"}\n",
        "preset_weight = \"\" \n",
        "\n",
        "def swap_models(model_1, model_2):\n",
        "    model_1, model_2 = model_2, model_1\n",
        "    return model_1, model_2\n",
        "\n",
        "if swap_model:\n",
        "    model_1, model_2 = swap_models(model_1, model_2)\n",
        "\n",
        "if mode==\"use_preset\":\n",
        "  weight_str = preset_weight\n",
        "elif mode==\"manual_adjust\":\n",
        "  weights = [IN_00, IN_01, IN_02, IN_03, IN_04, IN_05, IN_06, IN_07, IN_08, IN_09, IN_10, IN_11, M_00, OUT_11, OUT_10, OUT_09, OUT_08, OUT_07, OUT_06, OUT_05, OUT_04, OUT_03, OUT_02, OUT_01, OUT_00]\n",
        "  weights_str = ','.join(map(str, weights))\n",
        "else:\n",
        "  weights_str = input_weight\n",
        "\n",
        "%cd /content/kohya-trainer/tools\n",
        "!python merge_block_weighted.py \\\n",
        "  {model_1} \\\n",
        "  {model_2} \\\n",
        "  --output {output} \\\n",
        "  --device=\"cuda\"\\\n",
        "  --base_alpha {base_alpha} \\\n",
        "  {f\"--verbose\" if verbose else \"\"}  \\\n",
        "  --weights {weights_str}"
      ],
      "metadata": {
        "id": "BxdkRZbtZh08",
        "cellView": "form",
        "outputId": "87fc5664-cf05-4eb9-8bf7-a6d00d41de24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer/tools\n",
            "loading /content/models/pastelmix.ckpt\n",
            "loading /content/models/hitokomoru-30000.ckpt\n",
            "Output file already exists. Overwrite? (y/n)\n",
            "y\n",
            "weighted 'model.diffusion_model.input_blocks.0.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.input_blocks.0.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.norm.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.norm.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.proj_in.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.proj_in.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.proj_out.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.proj_out.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.norm.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.norm.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.proj_in.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.proj_in.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.proj_out.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.proj_out.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.input_blocks.3.0.op.bias': 0.75\n",
            "weighted 'model.diffusion_model.input_blocks.3.0.op.weight': 0.75\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.skip_connection.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.0.skip_connection.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.norm.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.norm.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.proj_in.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.proj_in.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.proj_out.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.proj_out.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.norm.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.norm.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.proj_in.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.proj_in.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.proj_out.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.proj_out.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.input_blocks.6.0.op.bias': 0.5\n",
            "weighted 'model.diffusion_model.input_blocks.6.0.op.weight': 0.5\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.skip_connection.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.0.skip_connection.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.norm.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.norm.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.proj_in.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.proj_in.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.proj_out.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.proj_out.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.norm.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.norm.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.proj_in.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.proj_in.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.proj_out.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.proj_out.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.input_blocks.9.0.op.bias': 0.25\n",
            "weighted 'model.diffusion_model.input_blocks.9.0.op.weight': 0.25\n",
            "weighted 'model.diffusion_model.middle_block.0.emb_layers.1.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.emb_layers.1.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.in_layers.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.in_layers.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.in_layers.2.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.in_layers.2.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.out_layers.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.out_layers.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.out_layers.3.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.0.out_layers.3.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.norm.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.norm.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.proj_in.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.proj_in.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.proj_out.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.proj_out.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.emb_layers.1.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.emb_layers.1.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.in_layers.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.in_layers.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.in_layers.2.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.in_layers.2.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.out_layers.0.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.out_layers.0.weight': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.out_layers.3.bias': 0.0\n",
            "weighted 'model.diffusion_model.middle_block.2.out_layers.3.weight': 0.0\n",
            "weighted 'model.diffusion_model.out.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.out.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.out.2.bias': 1.0\n",
            "weighted 'model.diffusion_model.out.2.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.skip_connection.bias': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.0.0.skip_connection.weight': 0.0833333333\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.skip_connection.bias': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.1.0.skip_connection.weight': 0.1666666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.skip_connection.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.0.skip_connection.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.norm.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.norm.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.proj_in.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.proj_in.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.proj_out.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.proj_out.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight': 0.9166666667\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.skip_connection.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.0.skip_connection.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.norm.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.norm.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.proj_in.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.proj_in.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.proj_out.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.proj_out.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight': 1.0\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.skip_connection.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.0.skip_connection.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.1.conv.bias': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.2.1.conv.weight': 0.25\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.skip_connection.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.0.skip_connection.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.norm.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.norm.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.proj_in.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.proj_in.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.proj_out.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.proj_out.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight': 0.3333333333\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.skip_connection.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.0.skip_connection.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.norm.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.norm.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.proj_in.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.proj_in.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.proj_out.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.proj_out.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight': 0.4166666667\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.skip_connection.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.0.skip_connection.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.norm.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.norm.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.proj_in.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.proj_in.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.proj_out.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.proj_out.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.2.conv.bias': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.5.2.conv.weight': 0.5\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.skip_connection.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.0.skip_connection.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.norm.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.norm.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.proj_in.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.proj_in.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.proj_out.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.proj_out.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight': 0.5833333333\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.skip_connection.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.0.skip_connection.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.norm.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.norm.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.proj_in.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.proj_in.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.proj_out.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.proj_out.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight': 0.6666666667\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.skip_connection.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.0.skip_connection.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.norm.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.norm.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.proj_in.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.proj_in.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.proj_out.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.proj_out.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.2.conv.bias': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.8.2.conv.weight': 0.75\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.skip_connection.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.0.skip_connection.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.norm.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.norm.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.proj_in.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.proj_in.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.proj_out.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.proj_out.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias': 0.8333333333\n",
            "weighted 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight': 0.8333333333\n",
            "weighted 'model.diffusion_model.time_embed.0.bias': 1.0\n",
            "weighted 'model.diffusion_model.time_embed.0.weight': 1.0\n",
            "weighted 'model.diffusion_model.time_embed.2.bias': 1.0\n",
            "weighted 'model.diffusion_model.time_embed.2.weight': 1.0\n",
            "Stage 2/2: 100% 1819/1819 [00:00<00:00, 2441420.47it/s]\n",
            "Saving...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 6.1. Inference\n",
        "%store -r\n",
        "\n",
        "v2 = False #@param {type:\"boolean\"}\n",
        "v_parameterization = False #@param {type:\"boolean\"}\n",
        "prompt = \"masterpiece, best quality, high quality, 1girl, solo, sitting, confident expression, long blonde hair, blue eyes, formal dress, jewelry, make-up, luxury, close-up, face, upper body.\" #@param {type: \"string\"}\n",
        "negative = \"worst quality, low quality, medium quality, deleted, lowres, comic, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry\" #@param {type: \"string\"}\n",
        "model = \"/content/models/new-pruned.ckpt\" #@param {type: \"string\"}\n",
        "vae = \"/content/vae/anime.vae.pt\" #@param {type: \"string\"}\n",
        "outdir = \"/content/tmp\" #@param {type: \"string\"}\n",
        "scale = 7 #@param {type: \"slider\", min: 1, max: 40}\n",
        "sampler = \"ddim\" #@param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "steps = 28 #@param {type: \"slider\", min: 1, max: 100}\n",
        "precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "width = 512 #@param {type: \"integer\"}\n",
        "height = 768 #@param {type: \"integer\"}\n",
        "images_per_prompt = 4 #@param {type: \"integer\"}\n",
        "batch_size = 4 #@param {type: \"integer\"}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 40}\n",
        "seed = -1 #@param {type: \"integer\"}\n",
        "\n",
        "final_prompt = f\"{prompt} --n {negative}\"\n",
        "\n",
        "!python /content/kohya-trainer/gen_img_diffusers.py \\\n",
        "  {\"--v2\" if v2 else \"\"} \\\n",
        "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
        "  --ckpt={model} \\\n",
        "  --outdir={outdir} \\\n",
        "  --xformers \\\n",
        "  {\"--vae=\" + vae if vae else \"\"} \\\n",
        "  --{precision} \\\n",
        "  --W={width} \\\n",
        "  --H={height} \\\n",
        "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
        "  --scale={scale} \\\n",
        "  --sampler={sampler} \\\n",
        "  --steps={steps} \\\n",
        "  --max_embeddings_multiples=3 \\\n",
        "  --batch_size={batch_size} \\\n",
        "  --images_per_prompt={images_per_prompt} \\\n",
        "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
        "  --prompt=\"{final_prompt}\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Eaz-xjYTZVBs",
        "outputId": "58ef8b16-2802-4d7d-e087-28725417a72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-27 02:54:49.424111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-01-27 02:54:50.266268: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-01-27 02:54:51.677503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-27 02:54:51.677711: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-27 02:54:51.677742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "load StableDiffusion checkpoint\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "loading text encoder: <All keys matched successfully>\n",
            "load VAE: /content/vae/anime.vae.pt\n",
            "additional VAE loaded\n",
            "Replace CrossAttention.forward to use NAI style Hypernetwork and xformers\n",
            "loading tokenizer\n",
            "Downloading (…)olve/main/vocab.json: 100% 961k/961k [00:05<00:00, 174kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 525k/525k [00:02<00:00, 237kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 155kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 905/905 [00:00<00:00, 259kB/s]\n",
            "pipeline is ready.\n",
            "iteration 1/1\n",
            "prompt 1/1: masterpiece, best quality, high quality, 1girl, solo, sitting, confident expression, long blonde hair, blue eyes, formal dress, jewelry, make-up, luxury, close-up, face, upper body.\n",
            "negative prompt: worst quality, low quality, medium quality, deleted, lowres, comic, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, jpeg artifacts, signature, watermark, username, blurry\n",
            "100% 28/28 [00:26<00:00,  1.07it/s]\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Conversion"
      ],
      "metadata": {
        "id": "WDPfF4uc5pd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#@title ## 2.1. Model Pruner\n",
        "%cd /content/kohya-trainer/tools\n",
        "if os.path.exists('prune.py'):\n",
        "  pass\n",
        "else:\n",
        "  # Add a comment to explain what the code is doing\n",
        "  # Download the pruning script if it doesn't already exist\n",
        "  !wget https://raw.githubusercontent.com/lopho/stable-diffusion-prune/main/prune.py\n",
        "\n",
        "\n",
        "#@markdown Convert to Float16\n",
        "fp16 = False #@param {'type':'boolean'}\n",
        "#@markdown Use EMA for weights\n",
        "ema = False #@param {'type':'boolean'}\n",
        "#@markdown Strip CLIP weights\n",
        "no_clip = False #@param {'type':'boolean'}\n",
        "#@markdown Strip VAE weights\n",
        "no_vae = False #@param {'type':'boolean'}\n",
        "#@markdown Strip depth model weights\n",
        "no_depth = False #@param {'type':'boolean'}\n",
        "#@markdown Strip UNet weights\n",
        "no_unet = False #@param {'type':'boolean'}\n",
        "#@markdown You need to input model ends with `.ckpt`, because `.safetensors` model won't work.\n",
        "\n",
        "input = \"/content/models/new.ckpt-00-bw.ckpt\" #@param {'type' : 'string'}\n",
        "\n",
        "\n",
        "# Notify the user that the model is being loaded\n",
        "print(f\"Loading model from {input}\")\n",
        "\n",
        "input_path = os.path.dirname(input)\n",
        "base_name = os.path.basename(input)\n",
        "output_name = base_name.split('.')[0]\n",
        "# Notify the user of the arguments being used\n",
        "if fp16:\n",
        "    print(\"Converting to float16\")\n",
        "    output_name += '-fp16'\n",
        "if ema:\n",
        "    print(\"Using EMA for weights\")\n",
        "    output_name += '-ema'\n",
        "if no_clip:\n",
        "    print(\"Stripping CLIP weights\")\n",
        "    output_name += '-no-clip'\n",
        "if no_vae:\n",
        "    print(\"Stripping VAE weights\")\n",
        "    output_name += '-no-vae'\n",
        "if no_depth:\n",
        "    print(\"Stripping depth model weights\")\n",
        "    output_name += '-no-depth'\n",
        "if no_unet:\n",
        "    print(\"Stripping UNet weights\")\n",
        "    output_name += '-no-unet'\n",
        "output_name += '-pruned'\n",
        "output_path = os.path.join(input_path, output_name + '.ckpt')\n",
        "\n",
        "\n",
        "!python3 prune.py \"{input}\" \\\n",
        "  \"{output_path}\" \\\n",
        "  {'--fp16' if fp16 else ''} \\\n",
        "  {'--ema' if ema else ''} \\\n",
        "  {'--no-clip' if no_clip else ''} \\\n",
        "  {'--no-vae' if no_vae else ''} \\\n",
        "  {'--no-depth' if no_depth else ''} \\\n",
        "  {'--no-unet' if no_unet else ''}\n",
        "\n",
        "# Notify the user of the output file location\n",
        "print(f\"Saving pruned model to {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fb3rxuCaSYta",
        "outputId": "49403e01-3e0d-416b-981b-b115c84f256f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer/tools\n",
            "Loading model from /content/models/new.ckpt-00-bw.ckpt\n",
            "Saving pruned model to /content/models/new-pruned.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.2. Convert Diffusers to `.ckpt/.safetensors`\n",
        "\n",
        "%cd /content/sd-notebook-collection/script/\n",
        "\n",
        "#@markdown ## Define model path\n",
        "weight = \"/content/models/AbyssOrangeMix2_sfw-pruned.ckpt\" #@param {'type': 'string'}\n",
        "weight_dir = os.path.dirname(weight)\n",
        "base_name = os.path.splitext(os.path.basename(weight))[0]\n",
        "\n",
        "convert = \"ckpt_safetensors_to_diffusers\" #@param [\"diffusers_to_ckpt_safetensors\", \"ckpt_safetensors_to_diffusers\"] {'allow-input': false}\n",
        "#@markdown ___\n",
        "#@markdown ## Conversion Config\n",
        "#@markdown ___\n",
        "#@markdown ### Diffusers to `.ckpt/.safetensors`\n",
        "use_safetensors = False #@param {'type': 'boolean'}\n",
        "\n",
        "save_precision = \"--float\" #@param [\"--fp16\",\"--bf16\",\"--float\"] {'allow-input': false}\n",
        "\n",
        "#@markdown ### `.ckpt/.safetensors` to Diffusers\n",
        "#@markdown is your model v1 or v2 based Stable Diffusion Model\n",
        "version = \"--v1\" #@param [\"--v1\",\"--v2\"] {'allow-input': false}\n",
        "diffusers = os.path.join(weight_dir, base_name)\n",
        "\n",
        "#@markdown Additional file for diffusers\n",
        "feature_extractor = True #@param {'type': 'boolean'}\n",
        "safety_checker = True #@param {'type': 'boolean'}\n",
        "\n",
        "if use_safetensors:\n",
        "    checkpoint = str(diffusers)+\".safetensors\"\n",
        "else:\n",
        "    checkpoint = str(diffusers)+\".ckpt\"\n",
        "\n",
        "if version == \"--v1\":\n",
        "  reference_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "elif version == \"--v2\":\n",
        "  reference_model = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "if convert == \"diffusers_to_ckpt_safetensors\":\n",
        "    if not weight.endswith(\".ckpt\") or weight.endswith(\".safetensors\"):\n",
        "        !python convert_diffusers20_original_sd.py \\\n",
        "            \"{weight}\" \\\n",
        "            \"{checkpoint}\"\" \\\n",
        "            {save_precision}\n",
        "\n",
        "else:    \n",
        "    !python convert_diffusers20_original_sd.py \\\n",
        "        \"{weight}\" \\\n",
        "        \"{diffusers}\" \\\n",
        "        {version} \\\n",
        "        --reference_model {reference_model} \n",
        "\n",
        "    url1 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/preprocessor_config.json\"\n",
        "    url2 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/config.json\"\n",
        "    url3 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/pytorch_model.bin\"\n",
        "\n",
        "    if feature_extractor == True:\n",
        "      if not os.path.exists(str(diffusers)+'/feature_extractor'):\n",
        "        os.makedirs(str(diffusers)+'/feature_extractor')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/feature_extractor' -o 'preprocessor_config.json' {url1}\n",
        "\n",
        "    if safety_checker == True:\n",
        "      if not os.path.exists(str(diffusers)+'/safety_checker'):\n",
        "        os.makedirs(str(diffusers)+'/safety_checker')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'config.json' {url2}\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'pytorch_model.bin' {url3}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TdCb8_dSSzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.3. Replace VAE of Existing Model \n",
        "%cd /content/sd-notebook-collection/script\n",
        "\n",
        "#@markdown You need to input model ends with `.ckpt`, because `.safetensors` model won't work.\n",
        "\n",
        "target_model = \"/content/models/Anything-V3.ckpt\" #@param {'type': 'string'}\n",
        "target_vae = \"/content/vae/anime.vae.pt\" #@param {'type': 'string'}\n",
        "\n",
        "# get the base file name and directory\n",
        "base_name = os.path.basename(target_model)\n",
        "base_dir = os.path.dirname(target_model)\n",
        "\n",
        "# get the file name without extension\n",
        "file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "# create the new file name\n",
        "new_file_name = file_name + \"-vae-swapped\"\n",
        "\n",
        "# get the file extension\n",
        "file_ext = os.path.splitext(base_name)[1]\n",
        "\n",
        "# create the output file path\n",
        "output_model = os.path.join(base_dir, new_file_name + file_ext)\n",
        "\n",
        "!python merge_vae.py \\\n",
        "  {target_model} \\\n",
        "  {target_vae} \\\n",
        "  {output_model}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g2QfhhlfbGsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.4. Convert CKPT to Safetensors\n",
        "%cd /content/sd-notebook-collection/script\n",
        "\n",
        "target_model = \"/content/models/Anything-V3-vae-swapped.ckpt\" #@param {'type': 'string'}\n",
        "\n",
        "!python convert_to_safetensors.py \\\n",
        "  --input-model {target_model} \\\n",
        "  --device gpu"
      ],
      "metadata": {
        "cellView": "form",
        "id": "e73cbbgIb2cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Deployment "
      ],
      "metadata": {
        "id": "CM-8uH-77BU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "\n",
        "#@markdown 1. Of course, you need a Huggingface account first.\n",
        "#@markdown 2. To create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
        "\n",
        "write_token = \"your-write-token-here\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cxKMspPO7OrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Define your Huggingface Repo\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model repo didn't exist, it will automatically create your repo.\n",
        "model_name = \"Anything-V3-better-vae\" #@param{type:\"string\"}\n",
        "make_this_model_private = False #@param{type:\"boolean\"}\n",
        "clone_with_git = True #@param{type:\"boolean\"}\n",
        "\n",
        "model_repo = user['name']+\"/\"+model_name.strip()\n",
        "\n",
        "validate_repo_id(model_repo)\n",
        "\n",
        "if make_this_model_private:\n",
        "  private_repo = True\n",
        "else:\n",
        "  private_repo = False\n",
        "\n",
        "if model_name != \"\":\n",
        "  try:\n",
        "      api.create_repo(repo_id=model_repo, \n",
        "                      private=private_repo)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if clone_with_git:\n",
        "  !git lfs uninstall\n",
        "\n",
        "  if model_name != \"\":\n",
        "    !git clone https://huggingface.co/{model_repo} /content/{model_name}\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "QTXsM170GUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Upload to Huggingface"
      ],
      "metadata": {
        "id": "yUNkWbMHcbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 3.3.1. Quick Upload to Huggingface\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be uploaded to model repo\n",
        "#@markdown You can't upload 7G model using `hf_hub` in Colab, please use `!git commit` cell below\n",
        "\n",
        "model_path = \"/content/models/Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "path_in_repo = \"Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "\n",
        "is_diffusers_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"feat: upload Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "\n",
        "\n",
        "if model_path != \"\":\n",
        "  path_obj = Path(model_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  if model_path.endswith(\".ckpt\") or model_path.endswith(\".safetensors\") or model_path.endswith(\".pt\"):\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "    \n",
        "    if path_in_repo != \"\":\n",
        "      path_in_repo = trained_model\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_path,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    \n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "  \n",
        "  elif is_diffusers_model == True:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=model_path,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\"\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
        "  \n",
        "  else:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=model_path,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\"\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pSUhgYLYdT2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 3.3.2. Or Commit using Git \n",
        "%cd /content/\n",
        "\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"your-email\" #@param {'type': 'string'}\n",
        "name = \"your-username\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_m = \"feat: upload Anything-V3-vae-swapped.safetensors\" #@param {'type': 'string'}\n",
        "\n",
        "%cd /content/{model_name}\n",
        "\n",
        "!git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles .\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "!git add .\n",
        "!git lfs help smudge\n",
        "!git commit -m \"{commit_m}\"\n",
        "!git push\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7bJev4PzOFFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
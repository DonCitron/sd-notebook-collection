{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/sd-notebook-collection/blob/main/sd-colab-toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Installation"
      ],
      "metadata": {
        "id": "akFjukqz5PbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.1 Install Dependencies\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "root_dir = \"/content/\"\n",
        "repo_dir = f\"{root_dir}/sd-scripts\"\n",
        "models_dir = f\"{root_dir}/models\"\n",
        "vaes_dir = f\"{root_dir}/vae\"\n",
        "deps_dir = f\"{root_dir}/deps\"\n",
        "\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "os.makedirs(vaes_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.isdir(repo_dir):\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "#@markdown This will install required Python packages\n",
        "!pip install --upgrade -r requirements.txt\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "def ubuntu_deps(url, name, dst):\n",
        "  !wget -q --show-progress {url}\n",
        "  with zipfile.ZipFile(name, 'r') as deps:\n",
        "    deps.extractall(dst)\n",
        "  !dpkg -i {dst}/*\n",
        "  os.remove(name)\n",
        "  shutil.rmtree(dst)\n",
        "ubuntu_deps(\"https://huggingface.co/Linaqruf/fast-repo/resolve/main/deb-libs.zip\", \"deb-libs.zip\", deps_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "0BicRIFqIjg0",
        "outputId": "f8967d4e-7193-4eba-f3f9-a7534a3c541d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sd-scripts'...\n",
            "remote: Enumerating objects: 1085, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 1085 (delta 59), reused 76 (delta 51), pack-reused 987\u001b[K\n",
            "Receiving objects: 100% (1085/1085), 2.50 MiB | 12.62 MiB/s, done.\n",
            "Resolving deltas: 100% (667/667), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/sd-scripts\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.15.0\n",
            "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.5/191.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.26.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy==6.1.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting albumentations==1.3.0\n",
            "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.7.0.68\n",
            "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.6.0\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers[torch]==0.10.2\n",
            "  Downloading diffusers-0.10.2-py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.1/503.1 KB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.9.0\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.35.0\n",
            "  Downloading bitsandbytes-0.35.0-py3-none-any.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.10.1\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors==0.2.6\n",
            "  Downloading safetensors-0.2.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.16.2\n",
            "  Downloading gradio-3.16.2-py3-none-any.whl (14.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair==4.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (4.2.2)\n",
            "Collecting easygui==0.98.3\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.28.2\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.6.12\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.13\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow==2.10.1\n",
            "  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub==0.12.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.15.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0->-r requirements.txt (line 2)) (3.9.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy==6.1.1->-r requirements.txt (line 3)) (0.2.6)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0->-r requirements.txt (line 4)) (0.0.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0->-r requirements.txt (line 4)) (0.18.3)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0->-r requirements.txt (line 4)) (4.7.0.68)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (6.0.0)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (4.4.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.9.0->-r requirements.txt (line 8)) (2023.1.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (1.51.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.10.1->-r requirements.txt (line 10)) (3.19.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (2.11.3)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (3.2.2)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (1.10.4)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (3.8.3)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (1.3.5)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio==3.16.2->-r requirements.txt (line 12)) (2.0.1)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.6-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair==4.2.2->-r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair==4.2.2->-r requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair==4.2.2->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests==2.28.2->-r requirements.txt (line 16)) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests==2.28.2->-r requirements.txt (line 16)) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests==2.28.2->-r requirements.txt (line 16)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests==2.28.2->-r requirements.txt (line 16)) (1.24.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.6.12->-r requirements.txt (line 17)) (0.14.1+cu116)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (3.1.0)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (23.1.21)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (0.2.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (0.30.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1->-r requirements.txt (line 21)) (15.0.6.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio==3.16.2->-r requirements.txt (line 12)) (1.8.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair==4.2.2->-r requirements.txt (line 13)) (5.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers[torch]==0.10.2->-r requirements.txt (line 7)) (3.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio==3.16.2->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio==3.16.2->-r requirements.txt (line 12)) (2022.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 4)) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 4)) (3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0->-r requirements.txt (line 4)) (2023.2.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio==3.16.2->-r requirements.txt (line 12)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio==3.16.2->-r requirements.txt (line 12)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio==3.16.2->-r requirements.txt (line 12)) (3.0.9)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.26.0,>=0.25.0\n",
            "  Downloading starlette-0.25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio==3.16.2->-r requirements.txt (line 12)) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 10)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 10)) (3.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0->-r requirements.txt (line 4)) (1.2.0)\n",
            "Building wheels for collected packages: fairscale, library, ffmpy, python-multipart\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332138 sha256=641c3fc41111019803864996587a378ad47707d39e5ebf3ca658435a3f1257f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/02/9b/dc7d4ff5145afdd28f456dae6605a46619af0370eca30d8d7e\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for library: filename=library-0.0.0-py3-none-any.whl size=37742 sha256=6750a6c544dff08057a1196c1c92f7afd55281582a0f06c74830a02d9be93c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/e6/3e/b5e380b0915aea5b9e43a10d0ea773f763683cfdeea2a4689c\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=92951249c977c05e8ac3d50c1d2200914fc60ce35e28cb2dbc129919dae348f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=cacd9b3c4ed226a43ded67935bad8021a2e456a3977b2a225a1a5310ca6c4949\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built fairscale library ffmpy python-multipart\n",
            "Installing collected packages: tokenizers, safetensors, rfc3986, pydub, library, keras, ffmpy, easygui, bitsandbytes, websockets, uc-micro-py, tensorflow-estimator, sniffio, requests, python-multipart, pycryptodome, orjson, numpy, mdurl, lightning-utilities, h11, ftfy, einops, aiofiles, uvicorn, torchmetrics, opencv-python, markdown-it-py, linkify-it-py, keras-preprocessing, huggingface-hub, fairscale, anyio, accelerate, transformers, timm, starlette, mdit-py-plugins, httpcore, diffusers, tensorboard, pytorch-lightning, httpx, fastapi, tensorflow, gradio, albumentations\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.2.1\n",
            "    Uninstalling albumentations-1.2.1:\n",
            "      Successfully uninstalled albumentations-1.2.1\n",
            "Successfully installed accelerate-0.15.0 aiofiles-23.1.0 albumentations-1.3.0 anyio-3.6.2 bitsandbytes-0.35.0 diffusers-0.10.2 easygui-0.98.3 einops-0.6.0 fairscale-0.4.13 fastapi-0.92.0 ffmpy-0.3.0 ftfy-6.1.1 gradio-3.16.2 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.12.0 keras-2.10.0 keras-preprocessing-1.1.2 library-0.0.0 lightning-utilities-0.6.0.post0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 numpy-1.22.4 opencv-python-4.7.0.68 orjson-3.8.6 pycryptodome-3.17 pydub-0.25.1 python-multipart-0.0.5 pytorch-lightning-1.9.0 requests-2.28.2 rfc3986-1.5.0 safetensors-0.2.6 sniffio-1.3.0 starlette-0.25.0 tensorboard-2.10.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0 timm-0.6.12 tokenizers-0.13.2 torchmetrics-0.11.1 transformers-4.26.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.28.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.3\n",
            "deb-libs.zip        100%[===================>]   1.49M  --.-KB/s    in 0.06s   \n",
            "Selecting previously unselected package aria2.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking aria2 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.2-2ubuntu0.20.04.1_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.2-2ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.1) ...\n",
            "Setting up lz4 (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Setting up liblz4-tool (1.9.2-2ubuntu0.20.04.1) ...\n",
            "Setting up aria2 (1.35.0-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.2. Download Available Model \n",
        "os.chdir(root_dir)\n",
        "import os\n",
        "\n",
        "installModels = []\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available model to download:\n",
        "\n",
        "modelUrl = [\"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-latest.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3-better-vae/resolve/main/any-v3-fp32-better-vae.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.0.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime.ckpt\", \\\n",
        "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/hesw23168/SD-Elysium-Model/resolve/main/Elysium_Anime_V2.ckpt\", \\\n",
        "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
        "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
        "modelList = [\"Animefull-final-latest\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Anything-V3\", \\\n",
        "             \"Anything-V3-pruned\", \\\n",
        "             \"Anything-V3-better-vae\", \\\n",
        "             \"Anything-V4\", \\\n",
        "             \"Anything-V4-pruned\", \\\n",
        "             \"Anything-V4-5-pruned\", \\\n",
        "             \"Kani-anime\", \\\n",
        "             \"Kani-anime-pruned\", \\\n",
        "             \"Elysium-anime-V2\", \\\n",
        "             \"OpenJourney-V2\", \\\n",
        "             \"Dreamlike-diffusion-V1-0\", \\\n",
        "             \"Stable-Diffusion-v1-5\"]\n",
        "modelName = \"Anything-V4\" #@param [\"Animefull-final-latest\", \"Animefull-final-pruned\", \"Anything-V3\", \"Anything-V3-pruned\", \"Anything-V3-better-vae\", \"Anything-V4\", \"Anything-V4-pruned\", \"Anything-V4-5-pruned\",\"Kani-anime\", \"Kani-anime-pruned\", \"Elysium-anime-V2\", \"OpenJourney-V2\", \"Dreamlike-diffusion-V1-0\", \"Stable-Diffusion-v1-5\"]\n",
        "\n",
        "installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  hf_token = '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' \n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {models_dir} -o {checkpoint_name}.ckpt \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "\n",
        "install_checkpoint()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UKP8CU-KNkjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.3. Download Custom Model\n",
        "\n",
        "os.chdir(models_dir)\n",
        "\n",
        "import os\n",
        "\n",
        "#@markdown ### Custom model\n",
        "modelList = []\n",
        "modelUrl = \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_sfw.ckpt\" #@param {'type': 'string'}\n",
        "modelUrl2 = \"\" #@param {'type': 'string'}\n",
        "modelUrl3 = \"\" #@param {'type': 'string'}\n",
        "modelUrl4 = \"\" #@param {'type': 'string'}\n",
        "\n",
        "modelList.extend([modelUrl, \n",
        "                  modelUrl2,\n",
        "                  modelUrl3,\n",
        "                  modelUrl4])\n",
        "\n",
        "def install_aria():\n",
        "  if not os.path.exists('/usr/bin/aria2c'):\n",
        "    !apt install -y -qq aria2\n",
        "\n",
        "def install(url):\n",
        "\n",
        "\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  \"{url}\"\n",
        "  elif url.startswith(\"magnet:?\"):\n",
        "    install_aria()\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 \"{url}\"\n",
        "  elif url.startswith(\"https://huggingface.co/\"):\n",
        "    base_name = os.path.basename(url)\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d models_dir -o \"{base_name}\" \"{url}\"\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d models_dir -Z \"{url}\"\n",
        "\n",
        "def install_checkpoint():\n",
        "  for customModel in modelList:\n",
        "    install(customModel)\n",
        "\n",
        "install_checkpoint()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A2NC0tm8vU-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1.4. Download VAE\n",
        "os.chdir(vaes_dir)\n",
        "\n",
        "installVae = []\n",
        "#@markdown ### Available VAE\n",
        "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
        "vaeUrl = [\"\", \\\n",
        "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
        "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
        "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
        "vaeList = [\"none\", \\\n",
        "           \"anime.vae.pt\", \\\n",
        "           \"waifudiffusion.vae.pt\", \\\n",
        "           \"stablediffusion.vae.pt\"]\n",
        "vaeName = \"anime.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
        "\n",
        "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
        "\n",
        "def install(vae_name, url):\n",
        "  hf_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {vaes_dir} -o {vae_name} \"{url}\"\n",
        "\n",
        "def install_vae():\n",
        "  if vaeName != \"none\":\n",
        "    for vae in installVae:\n",
        "      install(vae[0], vae[1])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "install_vae()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CZJLUBn3MgRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Conversion"
      ],
      "metadata": {
        "id": "WDPfF4uc5pd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#@title ## 2.1. Model Pruner\n",
        "\n",
        "%cd /content/sd-notebook-collection/script\n",
        "\n",
        "#@markdown Convert to Float16\n",
        "fp16 = False #@param {'type':'boolean'}\n",
        "#@markdown Use EMA for weights\n",
        "ema = False #@param {'type':'boolean'}\n",
        "#@markdown Strip CLIP weights\n",
        "no_clip = False #@param {'type':'boolean'}\n",
        "#@markdown Strip VAE weights\n",
        "no_vae = False #@param {'type':'boolean'}\n",
        "#@markdown Strip depth model weights\n",
        "no_depth = False #@param {'type':'boolean'}\n",
        "#@markdown Strip UNet weights\n",
        "no_unet = False #@param {'type':'boolean'}\n",
        "#@markdown You need to input model ends with `.ckpt`, because `.safetensors` model won't work.\n",
        "\n",
        "input = \"/content/models/AbyssOrangeMix2_sfw.ckpt\" #@param {'type' : 'string'}\n",
        "\n",
        "\n",
        "# Notify the user that the model is being loaded\n",
        "print(f\"Loading model from {input}\")\n",
        "\n",
        "input_path = os.path.dirname(input)\n",
        "base_name = os.path.basename(input)\n",
        "output_name = base_name.split('.')[0]\n",
        "# Notify the user of the arguments being used\n",
        "if fp16:\n",
        "    print(\"Converting to float16\")\n",
        "    output_name += '-fp16'\n",
        "if ema:\n",
        "    print(\"Using EMA for weights\")\n",
        "    output_name += '-ema'\n",
        "if no_clip:\n",
        "    print(\"Stripping CLIP weights\")\n",
        "    output_name += '-no-clip'\n",
        "if no_vae:\n",
        "    print(\"Stripping VAE weights\")\n",
        "    output_name += '-no-vae'\n",
        "if no_depth:\n",
        "    print(\"Stripping depth model weights\")\n",
        "    output_name += '-no-depth'\n",
        "if no_unet:\n",
        "    print(\"Stripping UNet weights\")\n",
        "    output_name += '-no-unet'\n",
        "output_name += '-pruned'\n",
        "output_path = os.path.join(input_path, output_name + '.ckpt')\n",
        "\n",
        "\n",
        "!python3 prune.py \"{input}\" \\\n",
        "  \"{output_path}\" \\\n",
        "  {'--fp16' if fp16 else ''} \\\n",
        "  {'--ema' if ema else ''} \\\n",
        "  {'--no-clip' if no_clip else ''} \\\n",
        "  {'--no-vae' if no_vae else ''} \\\n",
        "  {'--no-depth' if no_depth else ''} \\\n",
        "  {'--no-unet' if no_unet else ''}\n",
        "\n",
        "# Notify the user of the output file location\n",
        "print(f\"Saving pruned model to {output_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fb3rxuCaSYta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2.2. Convert Diffusers to `.ckpt/.safetensors`\n",
        "\n",
        "%cd /content/sd-notebook-collection/script/\n",
        "\n",
        "#@markdown ## Define model path\n",
        "weight = \"/content/models/AbyssOrangeMix2_sfw-pruned.ckpt\" #@param {'type': 'string'}\n",
        "weight_dir = os.path.dirname(weight)\n",
        "base_name = os.path.splitext(os.path.basename(weight))[0]\n",
        "\n",
        "convert = \"ckpt_safetensors_to_diffusers\" #@param [\"diffusers_to_ckpt_safetensors\", \"ckpt_safetensors_to_diffusers\"] {'allow-input': false}\n",
        "#@markdown ___\n",
        "#@markdown ## Conversion Config\n",
        "#@markdown ___\n",
        "#@markdown ### Diffusers to `.ckpt/.safetensors`\n",
        "use_safetensors = False #@param {'type': 'boolean'}\n",
        "\n",
        "save_precision = \"--float\" #@param [\"--fp16\",\"--bf16\",\"--float\"] {'allow-input': false}\n",
        "\n",
        "#@markdown ### `.ckpt/.safetensors` to Diffusers\n",
        "#@markdown is your model v1 or v2 based Stable Diffusion Model\n",
        "version = \"--v1\" #@param [\"--v1\",\"--v2\"] {'allow-input': false}\n",
        "diffusers = os.path.join(weight_dir, base_name)\n",
        "\n",
        "#@markdown Additional file for diffusers\n",
        "feature_extractor = True #@param {'type': 'boolean'}\n",
        "safety_checker = True #@param {'type': 'boolean'}\n",
        "\n",
        "if use_safetensors:\n",
        "    checkpoint = str(diffusers)+\".safetensors\"\n",
        "else:\n",
        "    checkpoint = str(diffusers)+\".ckpt\"\n",
        "\n",
        "if version == \"--v1\":\n",
        "  reference_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "elif version == \"--v2\":\n",
        "  reference_model = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "if convert == \"diffusers_to_ckpt_safetensors\":\n",
        "    if not weight.endswith(\".ckpt\") or weight.endswith(\".safetensors\"):\n",
        "        !python convert_diffusers20_original_sd.py \\\n",
        "            \"{weight}\" \\\n",
        "            \"{checkpoint}\"\" \\\n",
        "            {save_precision}\n",
        "\n",
        "else:    \n",
        "    !python convert_diffusers20_original_sd.py \\\n",
        "        \"{weight}\" \\\n",
        "        \"{diffusers}\" \\\n",
        "        {version} \\\n",
        "        --reference_model {reference_model} \n",
        "\n",
        "    url1 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/preprocessor_config.json\"\n",
        "    url2 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/config.json\"\n",
        "    url3 = \"https://huggingface.co/CompVis/stable-diffusion-safety-checker/resolve/main/pytorch_model.bin\"\n",
        "\n",
        "    if feature_extractor == True:\n",
        "      if not os.path.exists(str(diffusers)+'/feature_extractor'):\n",
        "        os.makedirs(str(diffusers)+'/feature_extractor')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/feature_extractor' -o 'preprocessor_config.json' {url1}\n",
        "\n",
        "    if safety_checker == True:\n",
        "      if not os.path.exists(str(diffusers)+'/safety_checker'):\n",
        "        os.makedirs(str(diffusers)+'/safety_checker')\n",
        "      \n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'config.json' {url2}\n",
        "      !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d '{diffusers}/safety_checker' -o 'pytorch_model.bin' {url3}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TdCb8_dSSzzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.3. Replace VAE of Existing Model \n",
        "%cd /content/sd-notebook-collection/script\n",
        "\n",
        "#@markdown You need to input model ends with `.ckpt`, because `.safetensors` model won't work.\n",
        "\n",
        "target_model = \"/content/models/Anything-V3.ckpt\" #@param {'type': 'string'}\n",
        "target_vae = \"/content/vae/anime.vae.pt\" #@param {'type': 'string'}\n",
        "\n",
        "# get the base file name and directory\n",
        "base_name = os.path.basename(target_model)\n",
        "base_dir = os.path.dirname(target_model)\n",
        "\n",
        "# get the file name without extension\n",
        "file_name = os.path.splitext(base_name)[0]\n",
        "\n",
        "# create the new file name\n",
        "new_file_name = file_name + \"-vae-swapped\"\n",
        "\n",
        "# get the file extension\n",
        "file_ext = os.path.splitext(base_name)[1]\n",
        "\n",
        "# create the output file path\n",
        "output_model = os.path.join(base_dir, new_file_name + file_ext)\n",
        "\n",
        "!python merge_vae.py \\\n",
        "  {target_model} \\\n",
        "  {target_vae} \\\n",
        "  {output_model}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g2QfhhlfbGsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.4. Convert CKPT to Safetensors\n",
        "%cd /content/sd-notebook-collection/script\n",
        "\n",
        "target_model = \"/content/models/Anything-V3-vae-swapped.ckpt\" #@param {'type': 'string'}\n",
        "\n",
        "!python convert_to_safetensors.py \\\n",
        "  --input-model {target_model} \\\n",
        "  --device gpu"
      ],
      "metadata": {
        "cellView": "form",
        "id": "e73cbbgIb2cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Deployment "
      ],
      "metadata": {
        "id": "CM-8uH-77BU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.1. Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "\n",
        "#@markdown 1. Of course, you need a Huggingface account first.\n",
        "#@markdown 2. To create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
        "\n",
        "write_token = \"your-write-token-here\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cxKMspPO7OrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3.2. Define your Huggingface Repo\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model repo didn't exist, it will automatically create your repo.\n",
        "model_name = \"Anything-V3-better-vae\" #@param{type:\"string\"}\n",
        "make_this_model_private = False #@param{type:\"boolean\"}\n",
        "clone_with_git = True #@param{type:\"boolean\"}\n",
        "\n",
        "model_repo = user['name']+\"/\"+model_name.strip()\n",
        "\n",
        "validate_repo_id(model_repo)\n",
        "\n",
        "if make_this_model_private:\n",
        "  private_repo = True\n",
        "else:\n",
        "  private_repo = False\n",
        "\n",
        "if model_name != \"\":\n",
        "  try:\n",
        "      api.create_repo(repo_id=model_repo, \n",
        "                      private=private_repo)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if clone_with_git:\n",
        "  !git lfs uninstall\n",
        "\n",
        "  if model_name != \"\":\n",
        "    !git clone https://huggingface.co/{model_repo} /content/{model_name}\n",
        "  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "QTXsM170GUpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Upload to Huggingface"
      ],
      "metadata": {
        "id": "yUNkWbMHcbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 3.3.1. Quick Upload to Huggingface\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be uploaded to model repo\n",
        "#@markdown You can't upload 7G model using `hf_hub` in Colab, please use `!git commit` cell below\n",
        "\n",
        "model_path = \"/content/models/Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "path_in_repo = \"Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "\n",
        "is_diffusers_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"feat: upload Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "\n",
        "\n",
        "if model_path != \"\":\n",
        "  path_obj = Path(model_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  if model_path.endswith(\".ckpt\") or model_path.endswith(\".safetensors\") or model_path.endswith(\".pt\"):\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "    \n",
        "    if path_in_repo != \"\":\n",
        "      path_in_repo = trained_model\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_path,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    \n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")\n",
        "  \n",
        "  elif is_diffusers_model == True:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=model_path,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\"\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/tree/main\\n\")\n",
        "  \n",
        "  else:\n",
        "    print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=model_path,\n",
        "        path_in_repo=trained_model,\n",
        "        repo_id=model_repo,\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\"\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pSUhgYLYdT2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### 3.3.2. Or Commit using Git \n",
        "%cd /content/\n",
        "\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"your-email\" #@param {'type': 'string'}\n",
        "name = \"your-username\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_m = \"feat: upload Anything-V3-vae-swapped.safetensors\" #@param {'type': 'string'}\n",
        "\n",
        "%cd /content/{model_name}\n",
        "\n",
        "!git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles .\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "!git add .\n",
        "!git lfs help smudge\n",
        "!git commit -m \"{commit_m}\"\n",
        "!git push\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7bJev4PzOFFB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}